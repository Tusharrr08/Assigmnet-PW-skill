{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7133f80-c54f-4466-aecd-2c35e0c3a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025d758-f109-450f-a76a-c41b61fc2c26",
   "metadata": {},
   "source": [
    "The main difference between the Euclidean distance metric and the Manhattan distance metric is how they calculate the distance between two data points.\n",
    "\n",
    "Euclidean distance is the square root of the sum of the squared differences between the feature values of two data points. This means that it gives more weight to larger differences in feature values.\n",
    "\n",
    "Manhattan distance is the sum of the absolute differences between the feature values of two data points. This means that it gives equal weight to all differences in feature values, regardless of size.\n",
    "\n",
    "The choice of distance metric can affect the performance of a KNN classifier or regressor in a few ways. For example, if the data has features with very different scales, then the Euclidean distance metric may give more weight to features with larger scales. This can lead to the KNN algorithm being biased towards those features.\n",
    "\n",
    "In general, the Manhattan distance metric is less sensitive to the scale of the features. This can make it a better choice for data with features with very different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c8d523-5c37-4d79-8286-ab8b9209b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc74c1-8543-4612-b0cb-46aa4e96f75b",
   "metadata": {},
   "source": [
    "There is no one-size-fits-all answer to the question of how to choose the optimal value of k for a KNN classifier or regressor. The best value of k will vary depending on the specific dataset and task.\n",
    "\n",
    "One common approach is to use cross-validation. This involves splitting the training data into multiple sets and training the KNN algorithm on each subset using a different value of k. The performance of the algorithm on each subset is then evaluated and the value of k that leads to the best overall performance is chosen.\n",
    "\n",
    "Another approach is to use a validation set. This involves splitting the training data into two sets: a training set and a validation set. The KNN algorithm is then trained on the training set and its performance is evaluated on the validation set. Different values of k are tried and the value that leads to the best performance on the validation set is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ac74f3-4868-4af6-9fc5-326e1faf56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f6636-3743-4b60-94ea-6be44a39968c",
   "metadata": {},
   "source": [
    "The choice of distance metric can affect the performance of a KNN classifier or regressor in a few ways. For example, if the data has features with very different scales, then the Euclidean distance metric may give more weight to features with larger scales. This can lead to the KNN algorithm being biased towards those features.\n",
    "\n",
    "In general, the Manhattan distance metric is less sensitive to the scale of the features. This can make it a better choice for data with features with very different scales.\n",
    "\n",
    "Another thing to consider is the type of noise in the data. If the data is noisy, then the Manhattan distance metric may be less sensitive to noise than the Euclidean distance metric.\n",
    "\n",
    "In general, you might choose the Manhattan distance metric over the Euclidean distance metric if:\n",
    "\n",
    "The data has features with very different scales.\n",
    "\n",
    "The data is noisy.\n",
    "\n",
    "You might choose the Euclidean distance metric over the Manhattan distance metric if:\n",
    "\n",
    "The data has features with similar scales.\n",
    "\n",
    "The data is not noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007b6838-117f-4c5d-8bba-e4e8c1acd747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8388a-5bee-447d-aacc-af97a14b370a",
   "metadata": {},
   "source": [
    "The most common hyperparameter in KNN classifiers and regressors is the value of k. This hyperparameter controls how many neighbors are used to make predictions.\n",
    "\n",
    "Other hyperparameters that can be tuned in KNN classifiers and regressors include:\n",
    "\n",
    "Distance metric: The distance metric used to calculate the distance between data points.\n",
    "\n",
    "Weighting scheme: The scheme used to weight the votes of the neighbors when making predictions.\n",
    "\n",
    "Feature scaling: Whether or not to scale the features before training the model.\n",
    "\n",
    "The best way to tune the hyperparameters of a KNN classifier or regressor is to use a validation set. This involves splitting the training data into two sets: a training set and a validation set. The KNN algorithm is then trained on the training set using different values of the hyperparameters. The performance of the algorithm on the validation set is then evaluated and the values of the hyperparameters that lead to the best performance are chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7866502-5306-48b7-b297-b3cfb2805882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51948a-03ae-40c3-bd69-77076dd43a1b",
   "metadata": {},
   "source": [
    "The size of the training set has a significant impact on the performance of a KNN classifier or regressor. In general, a larger training set will result in a more accurate model. This is because a larger training set will provide the model with more examples to learn from, which will help it to better understand the relationships between the features and the target variable.\n",
    "\n",
    "However, there is a point of diminishing returns when it comes to the size of the training set. Increasing the size of the training set beyond a certain point will not necessarily result in a significant improvement in accuracy, and it can also lead to overfitting.\n",
    "\n",
    "There are a few techniques that can be used to optimize the size of the training set for KNN:\n",
    "\n",
    "Use a validation set: A validation set is a subset of the training data that is used to evaluate the performance of the model during training. The model is trained on the training set and then evaluated on the validation set. This process is repeated until the model reaches a satisfactory level of performance on the validation set.\n",
    "\n",
    "Use dimensionality reduction: Dimensionality reduction is a technique that can be used to reduce the number of features in the training data without losing too much information. This can make the training data more manageable and can also help to improve the performance of the KNN model.\n",
    "\n",
    "Use a nearest neighbor search algorithm: There are a number of different nearest neighbor search algorithms that can be used to find the nearest neighbors of a given data point. Some of these algorithms are more efficient than others, especially for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "354ff15e-b111-49a0-ad48-830799d38b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d6603-f1ad-48f7-8ea9-f052a03dbe0d",
   "metadata": {},
   "source": [
    "Some potential drawbacks of using KNN as a classifier or regressor include:\n",
    "\n",
    "Computational cost: KNN can be computationally expensive, especially for large datasets. This is because the KNN algorithm needs to calculate the distance between the new data point and all of the data points in the training set.\n",
    "\n",
    "Sensitivity to noise and outliers: KNN can be sensitive to noise and outliers in the training data. This is because the KNN algorithm predicts the value of a new data point based on the values of its nearest neighbors. If the nearest neighbors are noisy or outliers, then the prediction will also be noisy or an outlier.\n",
    "\n",
    "Curse of dimensionality: KNN can suffer from the curse of dimensionality. This is because the distance between data points in high-dimensional spaces becomes less meaningful.\n",
    "\n",
    "Here are some techniques that can be used to overcome the drawbacks of KNN:\n",
    "\n",
    "Use a nearest neighbor search algorithm: As mentioned above, there are a number of different nearest neighbor search algorithms that can be used to find the nearest neighbors of a given data point. Some of these algorithms are more efficient than others, especially for large datasets.\n",
    "\n",
    "Use dimensionality reduction: Dimensionality reduction can be used to reduce the number of features in the training data without losing too much information. This can make the training data more manageable and can also help to improve the performance of the KNN model.\n",
    "\n",
    "Use a cross-validation technique: Cross-validation is a technique that can be used to evaluate the performance of a model on unseen data. This can help to prevent overfitting and can also help to identify the optimal value of the K parameter.\n",
    "\n",
    "Use an ensemble method: Ensemble methods combine the predictions of multiple machine learning models to produce a more accurate prediction. This can be useful for improving the performance of KNN, especially for complex datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c91b8b8-c182-42cd-acf9-b71c12e9010b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c6515-5435-4db2-a66d-a256cef5119e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b2189-5764-4458-a528-c69aa9543820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9395d9b-fe05-46e9-a31c-739fc20c5689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc2951-f557-4506-88df-06f468c74f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d19c7e-5a49-4452-97e2-84fdc84fdfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

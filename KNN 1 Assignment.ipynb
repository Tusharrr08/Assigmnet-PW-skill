{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0071b3ec-cdf9-483b-bf09-890589276ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453aabd-27ae-4945-b349-992497a05c07",
   "metadata": {},
   "source": [
    "The K-nearest neighbors (KNN) algorithm is a supervised machine learning algorithm that can be used for both classification and regression tasks. It works by finding the k most similar data points in the training set to a new data point, and then using the target values of those k data points to predict the target value of the new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4cb689-e008-46a6-864d-21b88739e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261fc3ff-53ee-4ad4-a329-cca674e07a40",
   "metadata": {},
   "source": [
    "The value of K is a hyperparameter that needs to be tuned for each specific dataset and task. There is no one-size-fits-all answer, but some common approaches include:\n",
    "\n",
    "Using cross-validation to try different values of K and select the one that performs best.\n",
    "\n",
    "Using a validation set to tune K.\n",
    "\n",
    "Using a heuristic, such as setting K to the square root of the number of training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eee9e2d-1138-4f66-9d48-568af60cade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5175610d-9bc1-4ebc-8ba2-0ce35e52aeb3",
   "metadata": {},
   "source": [
    "The only difference between the KNN classifier and the KNN regressor is how they predict the target value of the new data point.\n",
    "\n",
    "The KNN classifier predicts the most common target value among the k nearest neighbors.\n",
    "\n",
    "he KNN regressor predicts the average target value among the k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2f3a29-c603-4efb-a552-598e2d515a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e7d552-a95e-4575-b568-7d82f26e7cf2",
   "metadata": {},
   "source": [
    "The performance of KNN can be measured using a variety of metrics, depending on the type of task. For classification tasks, common metrics include accuracy, precision, recall, and F1 score. For regression tasks, common metrics include mean squared error (MSE) and root mean squared error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d4acc2-e08f-4414-a322-bc9b1b6cbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72888d4c-418f-48a6-9095-70586c83574e",
   "metadata": {},
   "source": [
    "The curse of dimensionality is a problem that can occur in machine learning algorithms, including KNN, when the number of features is high. The problem is that as the number of features increases, the distance between data points becomes less meaningful. This can make it difficult for KNN to find the k most similar data points to a new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5218a068-5f83-449d-8de3-65f32cd61a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db251105-484b-444f-9c6d-865f260fba9b",
   "metadata": {},
   "source": [
    "There are a few different ways to handle missing values in KNN. One common approach is to simply drop the data points with missing values. Another approach is to impute the missing values with a default value, such as the mean or median value of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c88561e-b735-4d5a-aea4-cd8074cfed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812919a8-5b97-46ed-a6b9-1e06958757b1",
   "metadata": {},
   "source": [
    "The KNN classifier and regressor generally perform similarly on classification and regression tasks, respectively. However, there are some cases where one may perform better than the other. For example, KNN regression may be better for tasks where the target value is continuous and has a wide range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13cbc92c-3006-4d6c-9fae-b254e174438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4079f5b-5594-470d-b7c6-7ceadf50f7fe",
   "metadata": {},
   "source": [
    "Strengths:\n",
    "\n",
    "KNN is a simple and intuitive algorithm.\n",
    "\n",
    "KNN is a non-parametric algorithm, which means that it does not make any assumptions about the distribution of the data.\n",
    "\n",
    "KNN can be used for both classification and regression tasks.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "KNN can be computationally expensive, especially for large datasets.\n",
    "\n",
    "KNN is sensitive to the choice of the value of K.\n",
    "\n",
    "KNN can be affected by the curse of dimensionality.\n",
    "\n",
    "To address the weaknesses:\n",
    "\n",
    "To address the computational cost of KNN, you can use techniques such as dimensionality reduction and efficient distance calculation algorithms.\n",
    "\n",
    "To address the sensitivity of KNN to the choice of K, you can use cross-validation or a validation set to tune K.\n",
    "\n",
    "To address the curse of dimensionality, you can use techniques such as feature selection and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72aa125f-70ca-436b-8dc3-bff2c9a3738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b3054-aef3-4c8c-a5f1-11e34995a13e",
   "metadata": {},
   "source": [
    "Euclidean distance is the most common distance metric used in KNN. It is calculated as the square root of the sum of the squared differences between the feature values of two data points.\n",
    "\n",
    "Manhattan distance is another distance metric that can be used in KNN. It is calculated as the sum of the absolute differences between the feature values of two data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d935d8ae-8348-4944-b3b3-a49e66b14cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2273d-74b7-47bb-8364-8e713333a73a",
   "metadata": {},
   "source": [
    "Feature scaling is a technique that can be used to normalize the feature values of a dataset. This can make KNN more robust to the curse of dimensionality and improve its performance.\n",
    "\n",
    "There are a variety of different feature scaling techniques that can be used, such as min-max scaling and standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364a146-4a65-4025-9fd3-2a7830b0367b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aeb27e-537c-43e1-8ceb-287d91048d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00a4f2-0f95-41ef-a9ea-041397370059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803e3d1-9b09-48d0-b926-bd4ded6d36f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5b3ce-b548-41e0-8fac-688a78e12e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

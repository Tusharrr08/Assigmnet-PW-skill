{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fa1065-8599-42d1-b245-bc72d79f2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ee746-13df-49ea-8d72-0772c8c1314e",
   "metadata": {},
   "source": [
    "\n",
    "An ensemble technique in machine learning is a method that combines the predictions of multiple machine learning models to produce a more accurate and robust prediction. Ensemble techniques can be used to improve the performance of any type of machine learning model, including regression, classification, and clustering models.\n",
    "\n",
    "Ensemble techniques work by reducing the variance and bias of individual models. Variance is the tendency of a model to make different predictions on the same data, while bias is the tendency of a model to make predictions that are consistently too high or too low. By combining the predictions of multiple models, ensemble techniques can reduce both variance and bias, resulting in more accurate and robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3830528d-4a01-4342-a387-bfaf020bee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb4ccb-d111-4a37-83c8-c94e7ff0a764",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for a number of reasons, including:\n",
    "\n",
    "Improved accuracy: Ensemble techniques can produce more accurate predictions than any individual model. This is because ensemble techniques reduce the variance and bias of individual models. Variance is the tendency of a model to make different predictions on the same data, while bias is the tendency of a model to make predictions that are consistently too high or too low. By combining the predictions of multiple models, ensemble techniques can reduce both variance and bias, resulting in more accurate and robust predictions.\n",
    "\n",
    "Reduced overfitting: Ensemble techniques are less likely to overfit the training data than individual models. Overfitting occurs when a model learns the training data too well and is unable to generalize to new data. Ensemble techniques reduce overfitting by combining the predictions of multiple models, which are trained on different subsets of the training data.\n",
    "\n",
    "Increased robustness: Ensemble techniques are more robust to noise and outliers in the training data. Noise is random variation in the data that can mislead the model, while outliers are data points that are very different from the rest of the data. Ensemble techniques are less affected by noise and outliers because they combine the predictions of multiple models, which are less likely to be misled by individual data points.\n",
    "\n",
    "Improved interpretability: Ensemble techniques can be used to identify the most important features for a machine learning task. This is because ensemble techniques can be used to calculate the weight of each feature in the final prediction. The features with the highest weights are the most important features for the machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d4a12d-d92f-4ba1-8853-091ea1d04cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7552f0e-4fff-4e25-94e6-b496340302c6",
   "metadata": {},
   "source": [
    "Bagging is an ensemble technique that combines the predictions of multiple machine learning models to produce a more accurate and robust prediction. Bagging works by training multiple models on different subsets of the training data, with replacement. This means that some data points may be included in multiple subsets, while other data points may not be included in any subsets.\n",
    "\n",
    "Once the models have been trained, their predictions are averaged to produce the final prediction. This averaging process helps to reduce the variance of the individual models, resulting in a more accurate and robust prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b701e643-1573-4016-b7c3-93b2bd11f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844a5d2-1631-4a33-94e8-17a0d63ddc81",
   "metadata": {},
   "source": [
    "Boosting is another ensemble technique that combines the predictions of multiple machine learning models to produce a more accurate and robust prediction. Boosting works by training multiple models sequentially, where each model is trained to focus on the errors of the previous model.\n",
    "\n",
    "The first model is trained on the original training data. The second model is trained on a weighted version of the training data, where the weights are assigned based on the errors of the first model. The third model is trained on a weighted version of the training data, where the weights are assigned based on the errors of the second model, and so on.\n",
    "\n",
    "Once the models have been trained, their predictions are weighted and combined to produce the final prediction. The weights are assigned based on the performance of the individual models on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15068adb-be07-4a74-9b2e-399b55665263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39207b-93a9-49a3-abc9-a5374e39b037",
   "metadata": {},
   "source": [
    "Improved accuracy: Ensemble techniques can produce more accurate predictions than any individual model.\n",
    "\n",
    "Reduced overfitting: Ensemble techniques are less likely to overfit the training data than individual models.\n",
    "\n",
    "Increased robustness: Ensemble techniques are more robust to noise and outliers in the training data.\n",
    "\n",
    "Improved interpretability: Ensemble techniques can be used to identify the most important features for a machine learning task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb19e6c-35c5-466f-8af4-a6b05cfd276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dfb41b-974d-4fc3-ace9-3c9309dfd140",
   "metadata": {},
   "source": [
    "Ensemble techniques are not always better than individual models. In some cases, a single model may be able to achieve better performance than an ensemble of models. However, in general, ensemble techniques are more likely to produce more accurate and robust predictions than individual models.\n",
    "\n",
    "Here are some cases where an individual model may be better than an ensemble of models:\n",
    "\n",
    "The training data is very small: Ensemble techniques can be more computationally expensive than using a single model. If the training data is very small, it may not be worth the computational cost to use an ensemble technique.\n",
    "\n",
    "The training data is very simple: If the training data is very simple, a single model may be able to learn the data perfectly. In this case, an ensemble technique may not be able to improve the performance of the model.\n",
    "\n",
    "The training data is very noisy: Ensemble techniques can be less robust to noise in the training data than individual models. If the training data is very noisy, a single model may be able to achieve better performance than an ensemble of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "995e627a-471e-4c1f-a142-ea19e16b22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adccac-cc3b-4b53-999e-9b5d222e73ec",
   "metadata": {},
   "source": [
    "\n",
    "To calculate the confidence interval using bootstrap steps are :\n",
    "\n",
    "Resample the data with replacement. This means that each data point has an equal chance of being selected, and some data points may be selected multiple times.\n",
    "\n",
    "Calculate the statistic of interest for the resampled data. This could be the mean, median, standard deviation, or any other statistic that is relevant to your research question.\n",
    "\n",
    "Repeat steps 1 and 2 many times, typically 1000 or more times.\n",
    "\n",
    "Sort the values of the statistic of interest for the resampled data.\n",
    "\n",
    "The confidence interval is the middle 95% of the sorted values. For example, if you resample the data 1000 times and calculate the mean each time, the 95% confidence interval would be the 25th and 975th values of the sorted list of means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9624d00-8057-47f4-afec-1df393ac3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2daf6-8de3-4e6b-9652-019fd82d9a5f",
   "metadata": {},
   "source": [
    "The bootstrap is a statistical method that can be used to estimate the distribution of a statistic. It works by resampling the data with replacement and calculating the statistic of interest for each resampled sample. The distribution of the statistic of interest for the resampled samples is then used to estimate the distribution of the statistic for the population.\n",
    "\n",
    "Steps involved in bootstrap:\n",
    "\n",
    "Resample the data with replacement. This means that each data point has an equal chance of being selected, and some data points may be selected multiple times.\n",
    "\n",
    "Calculate the statistic of interest for the resampled data. This could be the mean, median, standard deviation, or any other statistic that is relevant to your research question.\n",
    "\n",
    "Repeat steps 1 and 2 many times, typically 1000 or more times.\n",
    "\n",
    "Construct the distribution of the statistic of interest for the resampled data. This can be done by plotting the distribution or by calculating summary statistics, such as the mean, median, and standard deviation.\n",
    "\n",
    "Use the distribution of the statistic of interest for the resampled data to estimate the distribution of the statistic for the population. For example, if you are interested in estimating the mean of the population, you could calculate the mean of the statistic of interest for the resampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2418d062-355c-4333-85b0-1df83b86f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d541bfb6-ac97-4427-847a-e89f2d9aef76",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using bootstrap,  the steps are :\n",
    "\n",
    "Resample the sample data with replacement. This means that we randomly select a tree height from the sample and record it. We then put the tree back in the sample and randomly select another tree height. We repeat this process 1000 times to create a bootstrap sample of 1000 tree heights.\n",
    "\n",
    "Calculate the mean height of the bootstrap sample.\n",
    "\n",
    "Repeat steps 1 and 2 1000 times to create a distribution of 1000 bootstrap means.\n",
    "\n",
    "The 95% confidence interval for the population mean height is the middle 95% of the distribution of bootstrap means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebc7570d-6158-4f50-b58b-71bc6699badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Resample the sample data with replacement\n",
    "def bootstrap_sample(data):\n",
    "  bootstrapped_sample = []\n",
    "  for i in range(len(data)):\n",
    "    bootstrapped_sample.append(data[np.random.randint(len(data))])\n",
    "  return bootstrapped_sample\n",
    "\n",
    "# Calculate the mean height of the bootstrap sample\n",
    "def bootstrap_mean(data):\n",
    "  return np.mean(data)\n",
    "\n",
    "# Calculate the 95% confidence interval for the population mean height\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "  bootstrapped_means = []\n",
    "  for i in range(1000):\n",
    "    bootstrapped_sample = bootstrap_sample(data)\n",
    "    bootstrapped_mean = bootstrap_mean(bootstrapped_sample)\n",
    "    bootstrapped_means.append(bootstrapped_mean)\n",
    "\n",
    "  bootstrapped_means.sort()\n",
    "  lower_bound = bootstrapped_means[int(alpha / 2 * len(bootstrapped_means))]\n",
    "  upper_bound = bootstrapped_means[int((1 - alpha / 2) * len(bootstrapped_means))]\n",
    "\n",
    "  return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce4d5d3-13d4-4655-b91e-99bce30c926e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval for the population mean height: (13.8, 16.2)\n"
     ]
    }
   ],
   "source": [
    "sample_data = np.array([13, 14, 15, 16, 17])\n",
    "\n",
    "lower_bound, upper_bound = bootstrap_confidence_interval(sample_data)\n",
    "\n",
    "print(f\"95% confidence interval for the population mean height: ({lower_bound}, {upper_bound})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bbceb9-2c85-4bde-a9cd-807f6d134a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e39b5-a78f-44a9-95f0-a071e4e52967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75086db-9067-4964-820a-342efe933622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30388bb1-72fa-4bc6-9a14-14a2e66cabbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfca78-c5a3-45bd-99d6-4f53592ef52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98239eb-7a35-408a-a899-b487db894db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d3886-cbea-4df1-be99-4f183048e215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1add3c2-28ba-471f-9cca-31df4db3f0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc494a1-00ff-4765-bc30-d6afe47a4222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
